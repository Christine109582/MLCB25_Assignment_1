{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration and Preprocessing\n",
    "\n",
    "This notebook handles the data loading, preprocessing, and preparation for the machine learning task. We'll work with development and validation datasets, perform necessary cleaning operations, and prepare the data for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, let's import the necessary Python libraries for data manipulation and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define File Paths\n",
    "\n",
    "Set up the paths for input and output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from data/assignment1_dev_set.csv and data/assignment1_val_set.csv\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "a = \"../data/assignment1_dev_set.csv\"\n",
    "b = \"../data/assignment1_val_set.csv\"\n",
    "c = \"../data\"\n",
    "d = os.path.join(c, \"development_final_data.csv\")\n",
    "e = os.path.join(c, \"evaluation_final_data.csv\")\n",
    "\n",
    "if not os.path.exists(c):\n",
    "    os.makedirs(c)\n",
    "\n",
    "print(\"Loading from\", a, \"and\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data\n",
    "\n",
    "Load the development and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded.\n",
      "Shapes:\n",
      " Dev: (489, 140)\n",
      " Val: (211, 140)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    x = pd.read_csv(a, index_col=0)\n",
    "    y = pd.read_csv(b, index_col=0)\n",
    "    print(\"Data loaded.\")\n",
    "except FileNotFoundError as err:\n",
    "    print(\"Error:\", err)\n",
    "    print(\"Check if assignment1_dev_set.csv and assignment1_val_set.csv are in ./data\")\n",
    "    exit()\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\" Dev:\", x.shape)\n",
    "print(\" Val:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n",
    "\n",
    "### 4.1 Merge Datasets\n",
    "\n",
    "Combine development and validation sets for preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging data...\n",
      "Merged shape: (700, 141)\n"
     ]
    }
   ],
   "source": [
    "x[\"src\"] = \"dev\"\n",
    "y[\"src\"] = \"val\"\n",
    "\n",
    "print(\"Merging data...\")\n",
    "z = pd.concat([x, y], ignore_index=True)\n",
    "print(\"Merged shape:\", z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Feature Selection\n",
    "\n",
    "Remove unnecessary columns from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping columns ['Project ID', 'Experiment type', 'Disease MESH ID']\n"
     ]
    }
   ],
   "source": [
    "# Drop some columns\n",
    "cols = ['Project ID', 'Experiment type', 'Disease MESH ID']\n",
    "print(\"Dropping columns\", cols)\n",
    "z = z.drop(columns=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Handle Missing Values\n",
    "\n",
    "Impute missing values in numerical columns using median values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking missing in ['Host age', 'BMI']\n",
      "No missing values in ['Host age', 'BMI']\n"
     ]
    }
   ],
   "source": [
    "# Impute missing values for numerical columns\n",
    "nums = ['Host age', 'BMI']\n",
    "print(\"Checking missing in\", nums)\n",
    "m = z[nums].isnull().sum()\n",
    "if m.sum() > 0:\n",
    "    print(\"Missing values:\", m[m > 0])\n",
    "    for col in nums:\n",
    "        med = z[col].median()\n",
    "        z[col].fillna(med, inplace=True)\n",
    "    print(\"After fill:\", z[nums].isnull().sum())\n",
    "else:\n",
    "    print(\"No missing values in\", nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Categorical Encoding\n",
    "\n",
    "Convert categorical variables to numerical using one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding categorical column 'Sex'...\n",
      "Encoding done.\n"
     ]
    }
   ],
   "source": [
    "# Encode categorical\n",
    "print(\"Encoding categorical column 'Sex'...\")\n",
    "z = pd.get_dummies(z, columns=['Sex'], drop_first=True)\n",
    "print(\"Encoding done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Split Data Back\n",
    "\n",
    "Separate the preprocessed data back into development and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data back...\n",
      "Final shapes:\n",
      " Dev: (489, 137)\n",
      " Val: (211, 137)\n"
     ]
    }
   ],
   "source": [
    "print(\"Splitting data back...\")\n",
    "dev = z[z['src'] == 'dev'].copy()\n",
    "val = z[z['src'] == 'val'].copy()\n",
    "dev = dev.drop(columns=['src'])\n",
    "val = val.drop(columns=['src'])\n",
    "\n",
    "print(\"Final shapes:\")\n",
    "print(\" Dev:\", dev.shape)\n",
    "print(\" Val:\", val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Processed Data\n",
    "\n",
    "Save the preprocessed datasets to CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving files...\n",
      "Saved to data/development_final_data.csv and data/evaluation_final_data.csv\n",
      "Done. Note: Data not scaled.\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving files...\")\n",
    "try:\n",
    "    dev.to_csv(d, index=False)\n",
    "    val.to_csv(e, index=False)\n",
    "    print(\"Saved to\", d, \"and\", e)\n",
    "except Exception as ex:\n",
    "    print(\"Error saving files:\", ex)\n",
    "\n",
    "print(\"Done. Note: Data not scaled.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
